{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c8df328",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "<hr style = \"border:2px solid black\" ></hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93f50c3",
   "metadata": {},
   "source": [
    "\n",
    "**What?** k-Fold CV for Imbalanced Classification\n",
    "\n",
""   ]
  },
  {
   "cell_type": "markdown",
   "id": "10032aa7",
   "metadata": {},
   "source": [
    "# Class imbalance and CV\n",
    "<hr style = \"border:2px solid black\" ></hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6efb75",
   "metadata": {},
   "source": [
    "\n",
    "- The two most common approaches used for model evaluation are the train/test split and the k-fold cross-validation procedure. \n",
    "\n",
    "- Both approaches can be very effective in general, although they can result in misleading results and potentially fail when used on classification problems with a severe class imbalance. \n",
    "\n",
    "- Instead, the techniques must be modified to stratify the sampling by the class label, called stratified train-test split or **stratified k-fold cross-validation**.\n",
    "\n",
    "- The go-to validation set is a classical 10-fold CV but this can easily break down in the case of class imbalances, even if the skew is NOT extreme.\n",
    "    \n",
    "- The **reason** is that the data is split into k-folds with a uniform probability distribution. This might work fine for data with a balanced class distribution, but when the distribution is severely skewed, it is likely that one or more folds will have few or no examples from the minority class. \n",
    "\n",
""   ]
  },
  {
   "cell_type": "markdown",
   "id": "52af0b0f",
   "metadata": {},
   "source": [
    "# Import modules\n",
    "<hr style = \"border:2px solid black\" ></hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b0d222c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-27T09:37:12.891865Z",
     "start_time": "2022-02-27T09:37:12.889028Z"
    }
   },
   "outputs": [],
   "source": [
    "from numpy import unique\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570d9939",
   "metadata": {},
   "source": [
    "# Example of the issue\n",
    "<hr style = \"border:2px solid black\" ></hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4288c0",
   "metadata": {},
   "source": [
    "\n",
    "- Let's create a dataset with a 1:100 minority to majority class distribution.\n",
    "\n",
    "- A total of 10 examples in the minority class is not many. If we used 10-folds, we would get one example in each fold in the ideal case, which is not enough to train a model. For demonstration purposes, we will use 5-folds.\n",
    "\n",
    "- In the ideal case, we would have 10/5 or two examples in each fold, meaning 4*2 (8) folds worth of examples in a training dataset and 1*2 folds (2) in a given test dataset.\n",
    "\n",
""   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62bb4bd1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-27T09:19:34.506780Z",
     "start_time": "2022-02-27T09:19:34.500264Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Class=0 : 990/1000 (99.0%)\n",
      "> Class=1 : 10/1000 (1.0%)\n"
     ]
    }
   ],
   "source": [
    "# generate 2 class dataset\n",
    "X, y = make_classification(n_samples=1000, n_classes=2, weights=[\n",
    "                           0.99, 0.01], flip_y=0, random_state=1)\n",
    "# summarize dataset\n",
    "classes = unique(y)\n",
    "total = len(y)\n",
    "for c in classes:\n",
    "    n_examples = len(y[y == c])\n",
    "    percent = n_examples / total * 100\n",
    "    print('> Class=%d : %d/%d (%.1f%%)' % (c, n_examples, total, percent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47d6979b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-27T09:23:58.201804Z",
     "start_time": "2022-02-27T09:23:58.194400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">Train: 0=791, 1=9, Test: 0=199, 1=1\n",
      ">Train: 0=793, 1=7, Test: 0=197, 1=3\n",
      ">Train: 0=794, 1=6, Test: 0=196, 1=4\n",
      ">Train: 0=790, 1=10, Test: 0=200, 1=0\n",
      ">Train: 0=792, 1=8, Test: 0=198, 1=2\n"
     ]
    }
   ],
   "source": [
    "kfold = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "\n",
    "# enumerate the splits and summarize the distributions\n",
    "for train_ix, test_ix in kfold.split(X):\n",
    "    # select rows\n",
    "    train_X, test_X = X[train_ix], X[test_ix]\n",
    "    train_y, test_y = y[train_ix], y[test_ix]\n",
    "    # summarize train and test composition\n",
    "    train_0, train_1 = len(train_y[train_y == 0]), len(train_y[train_y == 1])\n",
    "    test_0, test_1 = len(test_y[test_y == 0]), len(test_y[test_y == 1])\n",
    "    print('>Train: 0=%d, 1=%d, Test: 0=%d, 1=%d' %\n",
    "          (train_0, train_1, test_0, test_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658aa10f",
   "metadata": {},
   "source": [
    "\n",
    "- We can see that in this case, there are some splits that have the expected 8/2 split for train and test sets, and others that are much worse, such as 6/4 (optimistic) and 10/0 (pessimistic).\n",
    "\n",
    "- Evaluating a model on these splits of the data would not give a reliable estimate of performance.\n",
    "\n",
    "- A similar issue exists if we use a simple 50/50 train/test split of the dataset, although the issue is less severe.\n",
    "    \n",
""   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "54be7278",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-27T09:28:20.591327Z",
     "start_time": "2022-02-27T09:28:20.586296Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">Train: 0=497, 1=3, Test: 0=493, 1=7\n"
     ]
    }
   ],
   "source": [
    "# split into train/test sets with same class ratio\n",
    "trainX, testX, trainy, testy = train_test_split(\n",
    "    X, y, test_size=0.5, random_state=2)\n",
    "# summarize\n",
    "train_0, train_1 = len(trainy[trainy == 0]), len(trainy[trainy == 1])\n",
    "test_0, test_1 = len(testy[testy == 0]), len(testy[testy == 1])\n",
    "print('>Train: 0=%d, 1=%d, Test: 0=%d, 1=%d' %\n",
    "      (train_0, train_1, test_0, test_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af98923",
   "metadata": {},
   "source": [
    "\n",
    "- Only three examples of the minority class are present in the training set, with seven in the test set.\n",
    "\n",
    "- Evaluating models on this split would not give them enough examples to learn from, too many to be evaluated on, and likely give poor performance. \n",
    "    \n",
    "- You can imagine how the situation could be worse with an even more severe random spit. \n",
    "\n",
""   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb276569",
   "metadata": {},
   "source": [
    "# Fixing the CV splitting\n",
    "<hr style = \"border:2px solid black\" ></hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf008817",
   "metadata": {},
   "source": [
    "\n",
    "- The **solution** is to not split the data randomly when using k-fold cross-validation or a train-test split.\n",
    "- Ee can use a version of k-fold cross-validation that preserves the imbalanced class distribution in each fold. It is called **stratified** k-fold cross-validation and will enforce the class distribution in each split of the data to match the distribution in the complete training dataset.\n",
    "    \n",
""   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ab23d9b1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-27T09:37:21.709123Z",
     "start_time": "2022-02-27T09:37:21.701512Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">Train: 0=792, 1=8, Test: 0=198, 1=2\n",
      ">Train: 0=792, 1=8, Test: 0=198, 1=2\n",
      ">Train: 0=792, 1=8, Test: 0=198, 1=2\n",
      ">Train: 0=792, 1=8, Test: 0=198, 1=2\n",
      ">Train: 0=792, 1=8, Test: 0=198, 1=2\n"
     ]
    }
   ],
   "source": [
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
    "# enumerate the splits and summarize the distributions\n",
    "for train_ix, test_ix in kfold.split(X, y):\n",
    "    # select rows\n",
    "    train_X, test_X = X[train_ix], X[test_ix]\n",
    "    train_y, test_y = y[train_ix], y[test_ix]\n",
    "    # summarize train and test composition\n",
    "    train_0, train_1 = len(train_y[train_y == 0]), len(train_y[train_y == 1])\n",
    "    test_0, test_1 = len(test_y[test_y == 0]), len(test_y[test_y == 1])\n",
    "    print('>Train: 0=%d, 1=%d, Test: 0=%d, 1=%d' %\n",
    "          (train_0, train_1, test_0, test_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78852141",
   "metadata": {},
   "source": [
    "# Other important considerations\n",
    "<hr style = \"border:2px solid black\" ></hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f0b196",
   "metadata": {},
   "source": [
    "\n",
    "- There are two things to keep in mind.\n",
    "\n",
    "- **First**, make sure you use a value of k in the k-fold that ensure an adequate number of examples in the train and test sets to fit and evaluate a model.\n",
    "\n",
    "- **Second**, make sure to use stratified k-fold CV with imbalanced datasets to preserve the class distribution in the train and test sets for each evaluation of a given model.\n",
    "\n",
""   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8b79085e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-27T09:52:23.709696Z",
     "start_time": "2022-02-27T09:52:23.704089Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 0=495, 1=5, Test: 0=495, 1=5\n"
     ]
    }
   ],
   "source": [
    "# split into train/test sets with same class ratio\n",
    "trainX, testX, trainy, testy = train_test_split(\n",
    "    X, y, test_size=0.5, random_state=2, stratify=y)\n",
    "\n",
    "# summarize\n",
    "train_0, train_1 = len(trainy[trainy == 0]), len(trainy[trainy == 1])\n",
    "test_0, test_1 = len(testy[testy == 0]), len(testy[testy == 1])\n",
    "\n",
    "print('Train: 0=%d, 1=%d, Test: 0=%d, 1=%d' %\n",
    "      (train_0, train_1, test_0, test_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda1fb2d",
   "metadata": {},
   "source": [
    "\n",
    "- Running the example creates a random split of the dataset into training and test sets, ensuring that the class distribution is preserved, in this case leaving five examples in each dataset. \n",
    "- Please note the argument `stratify=y`.\n",
    "    \n",
""   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25e2a72",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-27T09:44:04.029469Z",
     "start_time": "2022-02-27T09:44:04.026489Z"
    }
   },
   "source": [
    "# References\n",
    "<hr style = \"border:2px solid black\" ></hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78cb6c0",
   "metadata": {},
   "source": [
    "\n",
    "- https://machinelearningmastery.com/cross-validation-for-imbalanced-classification/\n",
    "- He, Haibo, and Yunqian Ma, eds. \"Imbalanced learning: foundations, algorithms, and applications.\" (2013).\n",
    "\n",
""   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c703ffaa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trainingAI",
   "language": "python",
   "name": "trainingai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
