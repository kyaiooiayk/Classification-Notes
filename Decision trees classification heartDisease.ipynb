{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "What? Decision tree classification applied to heart disease\n",
    "\n",
    "Corey Wade, Hands-On Gradient Boosting with XGBoost and scikit-learn\n",
    "https://github.com/PacktPublishing/Hands-On-Gradient-Boosting-with-XGBoost-and-Scikit-learn\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import pandas and numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem goal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "You have been asked by a hospital to use machine learning to predict heart disease. Your job is to develop a \n",
    "model and highlight two to three important features that doctors and nurses can focus on to improve patient \n",
    "health.\n",
    "\n",
    "You decide to use a decision tree classifier with fine-tuned hyperparameters. After the model has been built, \n",
    "you will interpret results using feature_importances_, an attribute that determines the most important features\n",
    "in predicting heart disease.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>145</td>\n",
       "      <td>233</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>130</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>187</td>\n",
       "      <td>0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130</td>\n",
       "      <td>204</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>172</td>\n",
       "      <td>0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>120</td>\n",
       "      <td>236</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>178</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "      <td>354</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>163</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  slope  \\\n",
       "0   63    1   3       145   233    1        0      150      0      2.3      0   \n",
       "1   37    1   2       130   250    0        1      187      0      3.5      0   \n",
       "2   41    0   1       130   204    0        0      172      0      1.4      2   \n",
       "3   56    1   1       120   236    0        1      178      0      0.8      2   \n",
       "4   57    0   0       120   354    0        1      163      1      0.6      2   \n",
       "\n",
       "   ca  thal  target  \n",
       "0   0     1       1  \n",
       "1   0     2       1  \n",
       "2   0     2       1  \n",
       "3   0     2       1  \n",
       "4   0     2       1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Upload heart.csv to dataFrame\n",
    "df_heart = pd.read_csv('../DATASETS/heart_disease.csv')\n",
    "\n",
    "# Show first five rows\n",
    "df_heart.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into X and y\n",
    "X = df_heart.iloc[:,:-1]\n",
    "y = df_heart.iloc[:,-1]\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the baseline accuracy with CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Baseline because we had perform no tuning\n",
    "Cross-validation provide the mean error across the fold. This provides a better estimate of the error.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: [0.74 0.85 0.77 0.73 0.7 ]\n",
      "Accuracy mean: 0.76\n"
     ]
    }
   ],
   "source": [
    "# Initialize Decision Tree Classifier\n",
    "model = DecisionTreeClassifier(random_state=2)\n",
    "\n",
    "# Obtain scores of cross-validation\n",
    "scores = cross_val_score(model, X, y, cv=5)\n",
    "\n",
    "# Display accuracy\n",
    "print('Accuracy:', np.round(scores, 2))\n",
    "\n",
    "# Display mean accuracy\n",
    "print('Accuracy mean: %0.2f' % (scores.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The initial accuracy is 76%. Let's see what gains can be made with hyperparameter fine-tuning.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomized_search_clf(params, runs=20, clf=DecisionTreeClassifier(random_state=2)):\n",
    "\n",
    "    # Instantiate GridSearchCV as grid_reg\n",
    "    rand_clf = RandomizedSearchCV(clf, params, n_iter=runs,\n",
    "                                  cv=5, n_jobs=-1, random_state=2)\n",
    "    # Fit grid_reg on X_train and y_train\n",
    "    rand_clf.fit(X_train, y_train)\n",
    "\n",
    "    # Extract best estimator\n",
    "    best_model = rand_clf.best_estimator_\n",
    "\n",
    "    # Extract best score\n",
    "    best_score = rand_clf.best_score_\n",
    "\n",
    "    # Print best score\n",
    "    print(\"Training score: {:.3f}\".format(best_score))\n",
    "\n",
    "    # Predict test set labels\n",
    "    y_pred = best_model.predict(X_test)\n",
    "\n",
    "    # Compute accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    # Print accuracy\n",
    "    print('Test score: {:.3f}'.format(accuracy))\n",
    "\n",
    "    # Return best model\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We create a dictionary where we give the range of the parametere we weant to try.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score: 0.797\n",
      "Test score: 0.855\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=8,\n",
       "            max_features=0.8, max_leaf_nodes=45, min_impurity_decrease=0.0,\n",
       "            min_impurity_split=None, min_samples_leaf=0.04,\n",
       "            min_samples_split=10, min_weight_fraction_leaf=0.05,\n",
       "            presort=False, random_state=2, splitter='best')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "randomized_search_clf(params={'criterion':['entropy', 'gini'],\n",
    "                              'splitter':['random', 'best'],\n",
    "                          'min_weight_fraction_leaf':[0.0, 0.0025, 0.005, 0.0075, 0.01],\n",
    "                          'min_samples_split':[2, 3, 4, 5, 6, 8, 10],\n",
    "                          'min_samples_leaf':[1, 0.01, 0.02, 0.03, 0.04],\n",
    "                          'min_impurity_decrease':[0.0, 0.0005, 0.005, 0.05, 0.10, 0.15, 0.2],\n",
    "                          'max_leaf_nodes':[10, 15, 20, 25, 30, 35, 40, 45, 50, None],\n",
    "                          'max_features':['auto', 0.95, 0.90, 0.85, 0.80, 0.75, 0.70],\n",
    "                          'max_depth':[None, 2,4,6,8],\n",
    "                          'min_weight_fraction_leaf':[0.0, 0.0025, 0.005, 0.0075, 0.01, 0.05]\n",
    "                         })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This is a definite improvement, and the model generalizes well on the test set. Let's see if we can do \n",
    "better by narrowing the range. “Another strategy is to stop checking hyperparameters whose defaults are \n",
    "working fine or the changes are pretty small.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score: 0.802\n",
      "Test score: 0.868\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=7,\n",
       "            max_features=0.78, max_leaf_nodes=45,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=0.045, min_samples_split=9,\n",
       "            min_weight_fraction_leaf=0.06, presort=False, random_state=2,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "randomized_search_clf(params={'max_depth':[None, 6, 7],\n",
    "'max_features':['auto', 0.78],\n",
    "'max_leaf_nodes':[45, None],\n",
    "'min_samples_leaf':[1, 0.035, 0.04, 0.045, 0.05],\n",
    "'min_samples_split':[2, 9, 10],\n",
    "'min_weight_fraction_leaf': [0.0, 0.05, 0.06, 0.07],\n",
    "},\n",
    "runs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This model is more accurate in the training and test score. However, for a proper baseline of comparison, \n",
    "it's essential to put the new model into cross_val_clf.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: [0.82 0.9  0.8  0.8  0.78]\n",
      "Accuracy mean: 0.82\n"
     ]
    }
   ],
   "source": [
    "# Initialize Decision Tree Classifier\n",
    "model = DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=7,\n",
    "            max_features=0.78, max_leaf_nodes=45,\n",
    "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "            min_samples_leaf=0.045, min_samples_split=9,\n",
    "            min_weight_fraction_leaf=0.06, presort=False, random_state=2,\n",
    "            splitter='best')\n",
    "\n",
    "# Obtain scores of cross-validation on the -->>WHOLE<<-- data set\n",
    "scores = cross_val_score(model, X, y, cv=5)\n",
    "\n",
    "# Display accuracy\n",
    "print('Accuracy:', np.round(scores, 2))\n",
    "\n",
    "# Display mean accuracy\n",
    "print('Accuracy mean: %0.2f' % (scores.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This is six percentage points higher than the default model. When it comes to predicting heart disease, \n",
    "more accuracy can save lives.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "When testing, it's important not to mix and match training and test sets. After a final model has been \n",
    "selected, HOWEVER, fitting the model on the entire dataset can be beneficial. Why? Because the goal is \n",
    "to test the model on data that has never been seen and fitting the model on the entire dataset may lead\n",
    "to additional gains in accuracy.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=7,\n",
       "            max_features=0.78, max_leaf_nodes=45,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=0.045, min_samples_split=9,\n",
       "            min_weight_fraction_leaf=0.06, presort=False, random_state=2,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_clf = DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=7,\n",
    "                       max_features=0.78, max_leaf_nodes=45,\n",
    "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                       min_samples_leaf=0.045, min_samples_split=9,\n",
    "                       min_weight_fraction_leaf=0.06, presort=False,\n",
    "                       random_state=2, splitter='best')\n",
    "best_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.04826754, 0.04081653, 0.48409586, 0.00568635, 0.        ,\n",
       "       0.        , 0.        , 0.00859483, 0.        , 0.02690379,\n",
       "       0.        , 0.18069065, 0.20494446])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_clf.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "It's not easy to interpret these results. The following code zips the columns along with the most important \n",
    "features into a dictionary before displaying them in reverse order for a clean output that is easy to interpret\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cp', 0.4840958610240171),\n",
       " ('thal', 0.20494445570568706),\n",
       " ('ca', 0.18069065321397942)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Zip columns and feature_importances_ into dict\n",
    "feature_dict = dict(zip(X.columns, best_clf.feature_importances_))\n",
    "\n",
    "# Sort dict by values (as list of tuples)\n",
    "sorted(feature_dict.items(), key=operator.itemgetter(1), reverse=True)[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "These numbers may be interpreted as their explanation of variance, so 'cp' accounts for 48% of the variance, \n",
    "which is more.\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
